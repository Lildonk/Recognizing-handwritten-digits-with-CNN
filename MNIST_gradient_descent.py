# -*- coding: utf-8 -*-
"""Copy of MNIST-Gradient-Descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nw4d-GbC21mK0y9uzbRy0fjgCuHHRmjt
"""

from keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy as np
import time

def plot_digit(image):
    """
    This function receives an image and plots the digit. 
    """
    plt.imshow(image, cmap='gray')
    plt.show()

# The x variables contain the images of handwritten digits the y variables contain their labels indicating 
# which digit is in the image. We will see an example of image and label shortly. We have two data sets here:
# training and test sets. The idea is that we use the training set to learn the function and then we evaluate 
# the system on images it did not see during training. This is to simulate the scenario where we build a system
# and we use it in the wild, where people write new digits and we would like our system to accurately recognize them.
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Each image is of size 28x28 and the training data set has 60,000 images
# the shape of x_train is (60000, 28, 28).

# Instead of using all 60,000 images, we will use "only" 20,000 in our experiments to speed up training.
training_size = 20000

# We will flatten the images to make our implementation easier. Instead of providing 
# an image of size 28x28 as input, we will provide a vector of size 784, with one entry 
# for each pixel of the image. We will also normalize the values in the images. The pixels
# values in the images vary from 0 to 255. We normalize them to avoid overflow. This will 
# be clear later once we better understand the learning algorithm.
images, labels = (x_train[0:training_size].reshape(training_size,28*28) / 255, y_train[0:training_size])

# The flattened images will be kept as column vectors in the matrix images
images = images.T
print('Shape of flattned images: ', images.shape)

# Here we apply the same transformations described above on the test data. 
images_test = x_test.reshape(x_test.shape[0], 28*28) / 255
images_test = images_test.T

def h_theta(X, W, B):
    """
    For a given matrix W and vector B, this function predicts the value
    of each digit for each image of X. Here we assume that each column of X
    is a flattened image. 
    """
    return W.dot(X) + B 

# Learning rate alpha, for controlling the step of gradient descent
alpha = 0.01

# Number of instances in the training set
m = images.shape[1]

# Matrix W initialized with zeros
W = np.zeros((10, 28*28))

# Matrix B also initialized with zeros
B = np.zeros((10,1))

# Creating Y matrix where each column is an one-hot vector
Y = np.zeros((10, m))
for index, value in enumerate(labels):
    Y[value][index] = 1

X = images
startt = time.time()
# Performs 1000 iterations of gradient descent
for i in range(1000):
  W = W - alpha * (1/m*(h_theta(X,W,B) - Y).dot(X.transpose()))
  d = np.expand_dims((1/m*(h_theta(X,W,B)-Y)).sum(axis=1),axis=1)
  B = B - alpha * d
endt = time.time()
print('Running time = ',endt-startt)
# Once finished performing 1000 gradient descent iterations, then compute the percentage
# of images from the test set classified correctly; print the percentage on the screen.
H = h_theta( images_test, W ,B)
lbls = []
for j in range(H.shape[1]):
  tmp = 0
  for i in range(H.shape[0]):
    if H[i][j]>=tmp:
      tmp = H[i][j]
      
  lbls.append(tmp)
count = 0
for i in range(len(lbls)):
  if (lbls[i]!=0 and y_test[i]!=0):
    count = count + 1
print('Rate of success:',(np.equal(np.argmax(h_theta(images_test,W,B),axis=0),y_test).sum()/y_test.shape*100)[0], '%')

for i in range(10):
    w = W[i,:].reshape(28, 28)
    plot_digit(w)
    
    
# Part of the code was given by Professor Levi Santana de Lelis in CMPUT 340 LAB4.
